## Idea:

## Main points
* Add auxiliary control tasks, e.g., pixel changes to learn a policy to maximize the control of environment. (more suitable for first-person case)
* Add reward prediction to overcome sparse reward and vlue function replay to faster value iteration.
* Experimentally, pixel control gives biggest improvement

## Experiments
* Datasets: Labyrinth, Atari
* Fixed mazes, goals changed, random maze
